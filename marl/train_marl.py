# train_marl.py
import os
import sys
import shutil
import json
import datetime
import random
import argparse

# Ajouter le r√©pertoire parent au PYTHONPATH pour permettre les imports
# Cela permet de lancer le script depuis n'importe quel r√©pertoire
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

import torch
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.vec_env import VecNormalize
from tensorboard import program
import threading
import time
from python.aero_patrol_wrapper import AeroPatrolWrapper
from python.config import (
    SAVE_DIR, MODEL_NAME, EPISODES, MAX_STEPS_PER_EPISODE,
    LEARNING_RATE, GAMMA, TENSORBOARD_LOG_DIR, NUM_DRONES,
    NUM_PARALLEL_ENVS, USE_VECENV, USE_SUBPROC_VECENV, UNITY_PORT,
    CURRICULUM_ENABLED, CURRICULUM_START_STAGE,
    CURRICULUM_STAGE0_SUCCESS_RATE_THRESHOLD, CURRICULUM_STAGE0_MIN_EPISODES, CURRICULUM_STAGE0_MIN_TIMESTEPS,
    CURRICULUM_STAGE0_DETECTION_RATE_THRESHOLD, CURRICULUM_STAGE0_TRACKING_RATE_THRESHOLD, CURRICULUM_STAGE0_STABILITY_THRESHOLD,
    CURRICULUM_STAGE1_SUCCESS_RATE_THRESHOLD, CURRICULUM_STAGE1_MIN_EPISODES, CURRICULUM_STAGE1_MIN_TIMESTEPS,
    CURRICULUM_STAGE1_DETECTION_RATE_THRESHOLD, CURRICULUM_STAGE1_TRACKING_RATE_THRESHOLD, CURRICULUM_STAGE1_STABILITY_THRESHOLD,
    CHECKPOINT_SAVE_FREQ, CHECKPOINT_DIR
)
from python.helpers import ensure_dir
import numpy as np


class TensorBoardCallback(BaseCallback):
    """
    Callback personnalis√© pour logger les m√©triques d√©taill√©es dans TensorBoard.
    """
    def __init__(self, verbose=0):
        super(TensorBoardCallback, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_info = []
        
    def _on_step(self) -> bool:
        # Les m√©triques standard sont d√©j√† logg√©es par Stable-Baselines3
        # Ce callback permet d'ajouter des m√©triques personnalis√©es
        
        # R√©cup√©rer les infos de l'environnement si disponibles
        infos = self.locals.get('infos', [])
        if len(infos) > 0 and infos[0] is not None:
            info = infos[0]
            
            # Logger les m√©triques de r√©compenses d√©taill√©es
            if 'coverage_reward' in info:
                self.logger.record('rewards/coverage', info['coverage_reward'])
            if 'detection_reward' in info:
                self.logger.record('rewards/detection', info['detection_reward'])
            if 'proximity_reward' in info:
                self.logger.record('rewards/proximity', info['proximity_reward'])  # Nouvelle m√©trique
            if 'tracking_reward' in info:
                self.logger.record('rewards/tracking', info['tracking_reward'])
            if 'capture_reward' in info:
                self.logger.record('rewards/capture', info['capture_reward'])
            if 'central_alert_reward' in info:
                self.logger.record('rewards/central_alert', info['central_alert_reward'])
            
            # Logger les p√©nalit√©s
            if 'collision_penalty' in info:
                self.logger.record('penalties/collision', info['collision_penalty'])
            if 'overlap_penalty' in info:
                self.logger.record('penalties/overlap', info['overlap_penalty'])
            if 'out_of_zone_penalty' in info:
                self.logger.record('penalties/out_of_zone', info['out_of_zone_penalty'])
            if 'obstacle_collision_penalty' in info:
                self.logger.record('penalties/obstacle_collision', info['obstacle_collision_penalty'])
            
            # Logger les m√©triques de performance
            if 'min_drone_distance' in info:
                self.logger.record('metrics/min_drone_distance', info['min_drone_distance'])
            if 'avg_drone_distance' in info:
                self.logger.record('metrics/avg_drone_distance', info['avg_drone_distance'])
            if 'min_distance_to_intruder' in info:
                self.logger.record('metrics/min_distance_to_intruder', info['min_distance_to_intruder'])  # Nouvelle m√©trique de diagnostic
            if 'coverage_ratio' in info:
                self.logger.record('metrics/coverage_ratio', info['coverage_ratio'])  # Nouvelle m√©trique de diagnostic
            if 'total_coverage_cells' in info:
                self.logger.record('metrics/total_coverage_cells', info['total_coverage_cells'])  # Nouvelle m√©trique de diagnostic
            if 'new_coverage_cells' in info:
                self.logger.record('metrics/new_coverage_cells', info['new_coverage_cells'])  # Nouvelle m√©trique de diagnostic
            if 'drones_out_of_zone' in info:
                self.logger.record('metrics/drones_out_of_zone', info['drones_out_of_zone'])
            if 'drones_near_obstacles' in info:
                self.logger.record('metrics/drones_near_obstacles', info['drones_near_obstacles'])
            if 'collision_count' in info:
                self.logger.record('metrics/collision_count', info['collision_count'])  # Nouvelle m√©trique
            if 'too_close_count' in info:
                self.logger.record('metrics/too_close_count', info['too_close_count'])  # Nouvelle m√©trique
            
            # Logger les m√©triques de curriculum learning
            if 'curriculum_stage' in info:
                self.logger.record('curriculum/stage', info['curriculum_stage'])
            if 'curriculum_episode_count' in info:
                self.logger.record('curriculum/episode_count', info['curriculum_episode_count'])
            if 'curriculum_episode_count_stage' in info:
                self.logger.record('curriculum/episode_count_stage', info['curriculum_episode_count_stage'])
            if 'curriculum_timesteps_count_stage' in info:
                self.logger.record('curriculum/timesteps_count_stage', info['curriculum_timesteps_count_stage'])
            if 'curriculum_success_rate' in info:
                self.logger.record('curriculum/success_rate', info['curriculum_success_rate'])
            if 'curriculum_detection_rate' in info:
                self.logger.record('curriculum/detection_rate', info['curriculum_detection_rate'])
            if 'curriculum_tracking_rate' in info:
                self.logger.record('curriculum/tracking_rate', info['curriculum_tracking_rate'])
            if 'curriculum_stability_cv' in info:
                self.logger.record('curriculum/stability_cv', info['curriculum_stability_cv'])
            
            # Logger la configuration du stage (pour voir ce qui est activ√©)
            if 'stage_enable_coverage' in info:
                self.logger.record('stage_config/enable_coverage', info['stage_enable_coverage'])
            if 'stage_enable_obstacles' in info:
                self.logger.record('stage_config/enable_obstacles', info['stage_enable_obstacles'])
            if 'stage_enable_zone' in info:
                self.logger.record('stage_config/enable_zone', info['stage_enable_zone'])
            if 'stage_enable_separation' in info:
                self.logger.record('stage_config/enable_separation', info['stage_enable_separation'])
            if 'stage_enable_central_alert' in info:
                self.logger.record('stage_config/enable_central_alert', info['stage_enable_central_alert'])
            if 'stage_enable_distance_penalty' in info:
                self.logger.record('stage_config/enable_distance_penalty', info['stage_enable_distance_penalty'])
            if 'stage_intruder_speed_mult' in info:
                self.logger.record('stage_config/intruder_speed_mult', info['stage_intruder_speed_mult'])
            if 'stage_radius_mult' in info:
                self.logger.record('stage_config/radius_mult', info['stage_radius_mult'])
        
        return True


def start_tensorboard(log_dir, port=6006):
    """
    D√©marre TensorBoard dans un thread s√©par√©.
    """
    def run_tensorboard():
        tb = program.TensorBoard()
        tb.configure(argv=[None, '--logdir', log_dir, '--port', str(port)])
        url = tb.launch()
        print(f"üìä TensorBoard d√©marr√© sur {url}")
    
    thread = threading.Thread(target=run_tensorboard, daemon=True)
    thread.start()
    time.sleep(2)  # Attendre que TensorBoard d√©marre
    return thread


def export_config_to_dict():
    """
    Exporte toute la configuration depuis config.py vers un dictionnaire.
    """
    import python.config as config_module
    import inspect
    
    config_dict = {}
    
    # R√©cup√©rer tous les attributs de config qui ne sont pas des m√©thodes ou priv√©s
    for attr_name in dir(config_module):
        if not attr_name.startswith('_'):
            try:
                attr_value = getattr(config_module, attr_name)
                # Ignorer les fonctions, m√©thodes et modules
                if not callable(attr_value) and not inspect.ismodule(attr_value):
                    # Convertir les types en JSON-serializable
                    if isinstance(attr_value, (int, float, str, bool, type(None))):
                        config_dict[attr_name] = attr_value
                    elif isinstance(attr_value, (list, tuple)):
                        config_dict[attr_name] = list(attr_value) if isinstance(attr_value, tuple) else attr_value
                    elif isinstance(attr_value, dict):
                        config_dict[attr_name] = attr_value
            except Exception:
                # Ignorer les attributs qui ne peuvent pas √™tre r√©cup√©r√©s
                pass
    
    return config_dict

def archive_experiment_files(experiment_dir, timestamp):
    """
    Archive les fichiers Python importants dans le dossier d'exp√©rimentation.
    """
    files_to_archive = [
        # Fichiers Python principaux
        os.path.join(parent_dir, "python", "config.py"),
        os.path.join(parent_dir, "python", "aero_patrol_wrapper.py"),
        os.path.join(parent_dir, "python", "env_manager.py"),
        os.path.join(current_dir, "train_marl.py"),
        os.path.join(current_dir, "evaluate_marl.py"),
    ]
    
    # Fichiers Unity (scripts C#) - v√©rifier si le dossier Assets existe
    unity_scripts = [
        os.path.join(parent_dir, "Assets", "Scripts", "EnvManager.cs"),
        os.path.join(parent_dir, "Assets", "Scripts", "IntruderAgent.cs"),
        os.path.join(parent_dir, "Assets", "Scripts", "DroneAgent.cs"),
        os.path.join(parent_dir, "Assets", "Scripts", "PatrolZone.cs"),
        os.path.join(parent_dir, "Assets", "Scripts", "ObstacleManager.cs"),
        os.path.join(parent_dir, "Assets", "PeacefulPie", "Scripts", "UnityComms.cs"),
    ]
    
    # Ajouter les fichiers Unity seulement s'ils existent
    for unity_file in unity_scripts:
        if os.path.exists(unity_file):
            files_to_archive.append(unity_file)
    
    archived_files = []
    for file_path in files_to_archive:
        if os.path.exists(file_path):
            try:
                file_name = os.path.basename(file_path)
                dest_path = os.path.join(experiment_dir, file_name)
                shutil.copy2(file_path, dest_path)
                archived_files.append(file_name)
                print(f"üì¶ Fichier archiv√© : {file_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Impossible d'archiver {file_path} : {e}")
        else:
            # Afficher un avertissement seulement pour les fichiers Unity (optionnels)
            if "Assets" in file_path:
                pass  # Les fichiers Unity sont optionnels
            else:
                print(f"‚ö†Ô∏è  Fichier non trouv√© : {file_path}")
    
    return archived_files

def generate_analysis_report(experiment_dir, timestamp, env):
    """
    G√©n√®re un rapport d'analyse dat√© en r√©f√©rence au run.
    """
    report_filename = f"rapport_analyse_{timestamp}.md"
    report_path = os.path.join(experiment_dir, report_filename)
    
    # R√©cup√©rer les informations du curriculum depuis l'environnement
    curriculum_info = {}
    if hasattr(env, 'envs') and len(env.envs) > 0:
        wrapped_env = env.envs[0]
        if hasattr(wrapped_env, 'env'):
            aero_env = wrapped_env.env
            if hasattr(aero_env, 'current_stage'):
                curriculum_info = {
                    'current_stage': aero_env.current_stage,
                    'episode_count': aero_env.episode_count,
                    'episode_count_per_stage': aero_env.episode_count_per_stage.copy() if hasattr(aero_env, 'episode_count_per_stage') else {},
                    'timesteps_count_per_stage': aero_env.timesteps_count_per_stage.copy() if hasattr(aero_env, 'timesteps_count_per_stage') else {},
                    'recent_episodes_success': len(aero_env.recent_episodes_success) if hasattr(aero_env, 'recent_episodes_success') else 0,
                    'recent_episodes_detection': len(aero_env.recent_episodes_detection) if hasattr(aero_env, 'recent_episodes_detection') else 0,
                    'recent_episodes_tracking': len(aero_env.recent_episodes_tracking) if hasattr(aero_env, 'recent_episodes_tracking') else 0,
                }
                
                # Calculer les success rates si disponibles
                if hasattr(aero_env, 'recent_episodes_success') and len(aero_env.recent_episodes_success) > 0:
                    curriculum_info['success_rate'] = float(np.mean(aero_env.recent_episodes_success))
                    curriculum_info['detection_rate'] = float(np.mean(aero_env.recent_episodes_detection))
                    curriculum_info['tracking_rate'] = float(np.mean(aero_env.recent_episodes_tracking))
    
    # G√©n√©rer le rapport
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"# Rapport d'Analyse - Run {timestamp}\n\n")
        f.write(f"**Date de g√©n√©ration** : {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("---\n\n")
        
        f.write("## 1. Configuration de l'Exp√©rimentation\n\n")
        f.write(f"- **Run ID** : `{timestamp}`\n")
        f.write(f"- **√âpisodes** : {EPISODES}\n")
        f.write(f"- **Steps par √©pisode** : {MAX_STEPS_PER_EPISODE}\n")
        f.write(f"- **Total timesteps** : {EPISODES * MAX_STEPS_PER_EPISODE}\n")
        f.write(f"- **Learning rate** : {LEARNING_RATE}\n")
        f.write(f"- **Gamma** : {GAMMA}\n")
        f.write(f"- **Nombre de drones** : {NUM_DRONES}\n\n")
        
        f.write("## 2. Curriculum Learning\n\n")
        f.write(f"- **Curriculum activ√©** : {CURRICULUM_ENABLED}\n")
        f.write(f"- **Stage de d√©part** : {CURRICULUM_START_STAGE}\n\n")
        
        if curriculum_info:
            f.write("### √âtat Final du Curriculum\n\n")
            f.write(f"- **Stage actuel** : {curriculum_info.get('current_stage', 'N/A')}\n")
            f.write(f"- **√âpisodes totaux** : {curriculum_info.get('episode_count', 0)}\n")
            f.write(f"- **√âpisodes par stage** :\n")
            for stage, count in curriculum_info.get('episode_count_per_stage', {}).items():
                f.write(f"  - Stage {stage} : {count} √©pisodes\n")
            f.write(f"- **Timesteps par stage** :\n")
            for stage, count in curriculum_info.get('timesteps_count_per_stage', {}).items():
                f.write(f"  - Stage {stage} : {count} timesteps\n")
            
            if 'success_rate' in curriculum_info:
                f.write(f"\n### M√©triques de Progression (Derniers {curriculum_info.get('recent_episodes_success', 0)} √©pisodes)\n\n")
                f.write(f"- **Success Rate (Capture)** : {curriculum_info['success_rate']:.2%}\n")
                f.write(f"- **Detection Rate** : {curriculum_info['detection_rate']:.2%}\n")
                f.write(f"- **Tracking Rate** : {curriculum_info['tracking_rate']:.2%}\n")
        
        f.write("\n### Crit√®res de Progression\n\n")
        f.write("#### Stage 0 ‚Üí Stage 1\n")
        f.write(f"- Success Rate >= {CURRICULUM_STAGE0_SUCCESS_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Detection Rate >= {CURRICULUM_STAGE0_DETECTION_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Tracking Rate >= {CURRICULUM_STAGE0_TRACKING_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Stabilit√© (CV) <= {CURRICULUM_STAGE0_STABILITY_THRESHOLD:.0%}\n")
        f.write(f"- Minimum √©pisodes : {CURRICULUM_STAGE0_MIN_EPISODES}\n")
        f.write(f"- Minimum timesteps : {CURRICULUM_STAGE0_MIN_TIMESTEPS}\n\n")
        
        f.write("#### Stage 1 ‚Üí Stage 2\n")
        f.write(f"- Success Rate >= {CURRICULUM_STAGE1_SUCCESS_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Detection Rate >= {CURRICULUM_STAGE1_DETECTION_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Tracking Rate >= {CURRICULUM_STAGE1_TRACKING_RATE_THRESHOLD:.0%}\n")
        f.write(f"- Stabilit√© (CV) <= {CURRICULUM_STAGE1_STABILITY_THRESHOLD:.0%}\n")
        f.write(f"- Minimum √©pisodes : {CURRICULUM_STAGE1_MIN_EPISODES}\n")
        f.write(f"- Minimum timesteps : {CURRICULUM_STAGE1_MIN_TIMESTEPS}\n\n")
        
        f.write("## 3. Configuration des Stages\n\n")
        f.write("### Stage 0: Focus Pursuit\n")
        f.write("- **Intrus vitesse** : 0.7x\n")
        f.write("- **Rayons** : 2.0x (d√©tection, tracking, capture)\n")
        f.write("- **Coverage** : ‚ùå D√©sactiv√©\n")
        f.write("- **Obstacles** : ‚ùå D√©sactiv√©\n")
        f.write("- **Zone** : ‚ùå D√©sactiv√©\n")
        f.write("- **S√©paration** : ‚ùå D√©sactiv√©\n")
        f.write("- **Central Alert** : ‚ùå D√©sactiv√©\n")
        f.write("- **Distance Penalty** : ‚úÖ Activ√©\n\n")
        
        f.write("### Stage 1: Obstacles + Zone\n")
        f.write("- **Intrus vitesse** : 0.9x\n")
        f.write("- **Rayons** : 1.2x\n")
        f.write("- **Coverage** : ‚ùå D√©sactiv√©\n")
        f.write("- **Obstacles** : ‚úÖ Activ√©\n")
        f.write("- **Zone** : ‚úÖ Activ√©\n")
        f.write("- **S√©paration** : ‚ùå D√©sactiv√©\n")
        f.write("- **Central Alert** : ‚úÖ Activ√©\n")
        f.write("- **Distance Penalty** : ‚úÖ Activ√©\n\n")
        
        f.write("### Stage 2: Complet\n")
        f.write("- **Intrus vitesse** : 1.0x\n")
        f.write("- **Rayons** : 1.0x\n")
        f.write("- **Coverage** : ‚úÖ Activ√©\n")
        f.write("- **Obstacles** : ‚úÖ Activ√©\n")
        f.write("- **Zone** : ‚úÖ Activ√©\n")
        f.write("- **S√©paration** : ‚úÖ Activ√©\n")
        f.write("- **Central Alert** : ‚úÖ Activ√©\n")
        f.write("- **Distance Penalty** : ‚úÖ Activ√©\n\n")
        
        f.write("## 4. Ajustements Appliqu√©s\n\n")
        f.write("### Modifications pour am√©liorer la d√©tection au Stage 0\n")
        f.write("- **Radius Multiplier Stage 0** : Augment√© de 1.5x √† **2.0x**\n")
        f.write("  - Rayon de d√©tection : 25.0 √ó 2.0 = **50.0 unit√©s** (au lieu de 37.5)\n")
        f.write("  - Objectif : Faciliter la d√©tection (distance moyenne observ√©e = 44.78)\n\n")
        f.write("- **Distance Penalty** : Augment√© de -0.01 √† **-0.02** (x2)\n")
        f.write("  - Objectif : Encourager l'approche de l'intrus\n\n")
        f.write("- **Proximity Reward** : Multiplicateurs augment√©s\n")
        f.write("  - Avant d√©tection : 0.2 ‚Üí **0.3** (x1.5)\n")
        f.write("  - Apr√®s d√©tection : 0.3 ‚Üí **0.45** (x1.5)\n")
        f.write("  - Max reward : 0.5 ‚Üí **0.75** (x1.5)\n\n")
        
        f.write("## 5. M√©triques TensorBoard\n\n")
        f.write("Les m√©triques suivantes sont disponibles dans TensorBoard :\n\n")
        f.write("### Curriculum\n")
        f.write("- `curriculum/stage` : Stage actuel\n")
        f.write("- `curriculum/episode_count` : Nombre total d'√©pisodes\n")
        f.write("- `curriculum/episode_count_stage` : √âpisodes dans le stage actuel\n")
        f.write("- `curriculum/timesteps_count_stage` : Timesteps dans le stage actuel\n")
        f.write("- `curriculum/success_rate` : Taux de succ√®s (capture)\n")
        f.write("- `curriculum/detection_rate` : Taux de d√©tection\n")
        f.write("- `curriculum/tracking_rate` : Taux de tracking\n")
        f.write("- `curriculum/stability_cv` : Coefficient de variation (stabilit√©)\n\n")
        
        f.write("### Configuration du Stage\n")
        f.write("- `stage_config/enable_coverage` : Coverage activ√© (1) ou non (0)\n")
        f.write("- `stage_config/enable_obstacles` : Obstacles activ√©s (1) ou non (0)\n")
        f.write("- `stage_config/enable_zone` : Zone activ√©e (1) ou non (0)\n")
        f.write("- `stage_config/enable_separation` : S√©paration activ√©e (1) ou non (0)\n")
        f.write("- `stage_config/enable_central_alert` : Central Alert activ√© (1) ou non (0)\n")
        f.write("- `stage_config/enable_distance_penalty` : Distance Penalty activ√© (1) ou non (0)\n")
        f.write("- `stage_config/intruder_speed_mult` : Multiplicateur de vitesse de l'intrus\n")
        f.write("- `stage_config/radius_mult` : Multiplicateur des rayons\n\n")
        
        f.write("## 6. Recommandations\n\n")
        f.write("1. **Surveiller les m√©triques de curriculum** dans TensorBoard pour suivre la progression\n")
        f.write("2. **V√©rifier les success rates** pour chaque stage avant la progression\n")
        f.write("3. **Analyser la stabilit√©** (CV) pour s'assurer d'un apprentissage stable\n")
        f.write("4. **Comparer les performances** entre les stages pour √©valuer l'efficacit√© du curriculum\n\n")
        
        f.write("---\n\n")
        f.write(f"*Rapport g√©n√©r√© automatiquement le {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n")
    
    print(f"üìä Rapport d'analyse g√©n√©r√© : {report_path}")
    return report_path


def save_experiment_info(experiment_dir, config_dict, model_info, timestamp):
    """
    Sauvegarde les informations de l'exp√©rimentation dans un fichier JSON.
    """
    experiment_info = {
        "timestamp": timestamp,
        "experiment_date": datetime.datetime.now().isoformat(),
        "model_info": model_info,
        "config": config_dict,
        "system_info": {
            "python_version": sys.version.split()[0],  # Version Python seulement
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
            "device_name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
            "device_memory_gb": torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else None,
        }
    }
    
    info_path = os.path.join(experiment_dir, "experiment_info.json")
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(experiment_info, f, indent=2, ensure_ascii=False)
    print(f"üìù Informations d'exp√©rimentation sauvegard√©es : {info_path}")
    
    return info_path

def train(load_model_path=None, reset_curriculum=False):
    """
    Fonction principale d'entra√Ænement.
    
    Args:
        load_model_path: Chemin vers un mod√®le √† charger (ex: "models/ppo_marl_20251204_115027.zip")
                       Si None, cr√©e un nouveau mod√®le
        reset_curriculum: Si True et load_model_path fourni, r√©initialise le curriculum √† z√©ro
                         Si False, continue le curriculum depuis l'√©tat sauvegard√©
    """
    # ======================================================================
    # üé≤ 0. FIXATION DES GRAINES POUR REPRODUCTIBILIT√â
    # ======================================================================
    SEED = 42  # Graine fixe pour reproductibilit√©
    random.seed(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(SEED)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"üé≤ Graines fix√©es pour reproductibilit√© (SEED={SEED})")
    
    # ======================================================================
    # üß© 1. CR√âATION DU DOSSIER D'EXP√âRIMENTATION
    # ======================================================================
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_dir = os.path.join(current_dir, "experiments", f"exp_{timestamp}")
    ensure_dir(experiment_dir)
    print(f"\nüöÄ Dossier d'exp√©rimentation cr√©√© : {experiment_dir}\n")
    
    # V√©rifier la disponibilit√© du GPU
    # Note: PPO avec MLP (Multi-Layer Perceptron) n'est pas optimal sur GPU
    # Stable-Baselines3 recommande d'utiliser CPU pour les politiques MLP
    # Pour CNN, GPU est recommand√©, mais nous utilisons MLP ici
    use_gpu_for_training = False  # D√©sactiver GPU pour PPO avec MLP (plus rapide sur CPU)
    
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        print(f"‚úÖ GPU d√©tect√© : {gpu_name} ({gpu_memory:.1f} GB)")
        print(f"‚úÖ CUDA version : {torch.version.cuda}")
        if use_gpu_for_training:
            print(f"‚úÖ PyTorch utilisera le GPU pour l'entra√Ænement")
        else:
            print(f"‚ÑπÔ∏è  GPU disponible mais utilisation du CPU recommand√©e pour PPO avec MLP")
            print(f"   (Le GPU est plus lent pour les politiques MLP selon Stable-Baselines3)")
            device = "cpu"
    else:
        print("‚ö†Ô∏è  Aucun GPU d√©tect√©. L'entra√Ænement utilisera le CPU.")
        device = "cpu"
    
    if use_gpu_for_training and torch.cuda.is_available():
        device = "cuda"
    
    # ======================================================================
    # ‚öôÔ∏è 2. EXPORT ET SAUVEGARDE DE LA CONFIGURATION
    # ======================================================================
    config_dict = export_config_to_dict()
    config_path = os.path.join(experiment_dir, "config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_dict, f, indent=2, ensure_ascii=False)
    print(f"üìù Configuration sauvegard√©e : {config_path}")
    
    # ======================================================================
    # üì¶ 3. ARCHIVAGE DES FICHIERS IMPORTANTS
    # ======================================================================
    print("\nüì¶ Archivage des fichiers importants...")
    archived_files = archive_experiment_files(experiment_dir, timestamp)
    print(f"‚úÖ {len(archived_files)} fichiers archiv√©s\n")
    
    # Cr√©e les dossiers de sauvegarde si n√©cessaire
    ensure_dir(SAVE_DIR)
    ensure_dir(TENSORBOARD_LOG_DIR)
    
    # Cr√©e un sous-dossier avec timestamp pour cette session d'entra√Ænement
    session_log_dir = os.path.join(TENSORBOARD_LOG_DIR, f"ppo_marl_{timestamp}")
    ensure_dir(session_log_dir)
    
    # Lier le dossier de logs TensorBoard au dossier d'exp√©rimentation
    # Cr√©er un lien symbolique ou copier les logs dans experiment_dir
    experiment_logs_dir = os.path.join(experiment_dir, "tensorboard_logs")
    # On cr√©era un fichier de r√©f√©rence plut√¥t qu'un lien (plus portable)
    logs_reference = {
        "tensorboard_log_dir": session_log_dir,
        "relative_path": os.path.relpath(session_log_dir, experiment_dir)
    }
    with open(os.path.join(experiment_dir, "logs_reference.json"), 'w') as f:
        json.dump(logs_reference, f, indent=2)
    
    print(f"üìä Logs TensorBoard : {session_log_dir}")
    
    # ======================================================================
    # üåê 3.5. CR√âATION DES ENVIRONNEMENTS (S√âQUENTIEL OU PARALL√àLE)
    # ======================================================================
    print("üîç [DIAGNOSTIC] D√©but de la cr√©ation des environnements...")
    sys.stdout.flush()  # Forcer l'affichage imm√©diat
    
    if USE_VECENV and NUM_PARALLEL_ENVS > 1:
        vecenv_type = "SubprocVecEnv" if USE_SUBPROC_VECENV else "DummyVecEnv"
        print(f"üöÄ Cr√©ation de {NUM_PARALLEL_ENVS} environnements parall√®les avec {vecenv_type}")
        sys.stdout.flush()
        
        def make_env(rank):
            """Cr√©e un environnement avec un port sp√©cifique (fonction picklable pour SubprocVecEnv)."""
            def _init():
                print(f"üîç [DIAGNOSTIC] Cr√©ation de l'environnement {rank} (port {UNITY_PORT + rank})...")
                sys.stdout.flush()
                try:
                    env = AeroPatrolWrapper(num_drones=NUM_DRONES, max_steps=MAX_STEPS_PER_EPISODE, port=UNITY_PORT + rank)
                    print(f"üîç [DIAGNOSTIC] Environnement {rank} cr√©√©, reset en cours...")
                    sys.stdout.flush()
                    env.reset(seed=SEED + rank)  # Seed diff√©rent par environnement pour diversit√©
                    print(f"üîç [DIAGNOSTIC] Environnement {rank} reset termin√©")
                    sys.stdout.flush()
                    env = Monitor(env, os.path.join(session_log_dir, f"env_{rank}"), allow_early_resets=True)
                    return env
                except Exception as e:
                    print(f"‚ùå [DIAGNOSTIC] Erreur lors de la cr√©ation de l'environnement {rank} : {e}")
                    sys.stdout.flush()
                    raise
            return _init
        
        # Cr√©er les environnements avec des ports diff√©rents (9000, 9001, etc.)
        print(f"üîç [DIAGNOSTIC] Cr√©ation des fonctions d'environnement...")
        sys.stdout.flush()
        env_fns = [make_env(i) for i in range(NUM_PARALLEL_ENVS)]
        
        if USE_SUBPROC_VECENV:
            # Vraie parall√©lisation avec processus s√©par√©s
            print(f"üîç [DIAGNOSTIC] Cr√©ation de SubprocVecEnv...")
            sys.stdout.flush()
            env = SubprocVecEnv(env_fns, start_method='spawn')
            print(f"‚úÖ {NUM_PARALLEL_ENVS} environnements cr√©√©s en parall√®le (SubprocVecEnv)")
            print(f"   ‚Üí Ports : {UNITY_PORT} √† {UNITY_PORT + NUM_PARALLEL_ENVS - 1}")
            print(f"   ‚Üí Chaque environnement dans son propre processus Python")
            print(f"   ‚ö†Ô∏è  Assurez-vous que {NUM_PARALLEL_ENVS} instances Unity sont lanc√©es sur ces ports")
        else:
            # Traitement s√©quentiel mais batch (DummyVecEnv)
            print(f"üîç [DIAGNOSTIC] Cr√©ation de DummyVecEnv...")
            sys.stdout.flush()
            env = DummyVecEnv(env_fns)
            print(f"‚úÖ {NUM_PARALLEL_ENVS} environnements cr√©√©s (DummyVecEnv - s√©quentiel)")
            print(f"   ‚Üí Ports : {UNITY_PORT} √† {UNITY_PORT + NUM_PARALLEL_ENVS - 1}")
            print(f"   ‚ö†Ô∏è  Assurez-vous que {NUM_PARALLEL_ENVS} instances Unity sont lanc√©es sur ces ports")
    else:
        print("üì¶ Cr√©ation d'un environnement unique (s√©quentiel)")
        sys.stdout.flush()
        print("üîç [DIAGNOSTIC] Cr√©ation de AeroPatrolWrapper...")
        sys.stdout.flush()
        # Initialise l'environnement MARL multi-drone
        env = AeroPatrolWrapper(num_drones=NUM_DRONES, max_steps=MAX_STEPS_PER_EPISODE)
        print("üîç [DIAGNOSTIC] AeroPatrolWrapper cr√©√©, reset en cours...")
        sys.stdout.flush()
        # Fixer la graine de l'environnement pour reproductibilit√©
        env.reset(seed=SEED)
        print("üîç [DIAGNOSTIC] Reset termin√©, cr√©ation du Monitor...")
        sys.stdout.flush()
        # Envelopper l'environnement avec Monitor pour logging
        env = Monitor(env, session_log_dir, allow_early_resets=True)
    
    print("üîç [DIAGNOSTIC] Environnements cr√©√©s avec succ√®s")
    sys.stdout.flush()
    
    # ======================================================================
    # ü§ñ 4. CHARGEMENT OU CR√âATION DU MOD√àLE
    # ======================================================================
    print("üîç [DIAGNOSTIC] D√©but de la section chargement/cr√©ation du mod√®le...")
    sys.stdout.flush()
    
    if load_model_path:
        # Normaliser le chemin du mod√®le
        if not os.path.isabs(load_model_path):
            # Chemin relatif : chercher dans models/
            models_dir = os.path.join(current_dir, SAVE_DIR)
            # Enlever "models/" du d√©but si pr√©sent pour √©viter duplication
            if load_model_path.startswith('models/'):
                load_model_path = load_model_path[7:]  # Enlever "models/"
            if not load_model_path.endswith('.zip'):
                load_model_path = f"{load_model_path}.zip"
            load_model_path = os.path.join(models_dir, load_model_path)
        
        if not os.path.exists(load_model_path):
            print(f"‚ùå Erreur : Mod√®le non trouv√© : {load_model_path}")
            print(f"   Recherche dans : {os.path.dirname(load_model_path)}")
            return
        
        print(f"üì¶ Chargement du mod√®le : {os.path.basename(load_model_path)}")
        sys.stdout.flush()
        try:
            print("üîç [DIAGNOSTIC] D√©but du chargement du mod√®le PPO...")
            sys.stdout.flush()
            model = PPO.load(load_model_path, env=env, device=device)
            print("‚úÖ Mod√®le charg√© avec succ√®s")
            sys.stdout.flush()
            
            # ‚ö†Ô∏è  IMPORTANT : Mettre √† jour le tensorboard_log pour √©viter les erreurs "No such file or directory"
            # Le mod√®le charg√© peut avoir un ancien chemin TensorBoard qui n'existe plus
            try:
                model.tensorboard_log = session_log_dir
                if hasattr(model, 'logger'):
                    # Fermer l'ancien logger s'il existe
                    if hasattr(model.logger, 'close'):
                        try:
                            model.logger.close()
                        except:
                            pass
                    # R√©initialiser le logger avec le nouveau chemin
                    from stable_baselines3.common.logger import configure
                    model.logger = configure(session_log_dir, ["stdout", "csv", "tensorboard"])
                print(f"üìä TensorBoard log mis √† jour : {session_log_dir}")
            except Exception as tb_error:
                print(f"‚ö†Ô∏è  Avertissement : Impossible de mettre √† jour TensorBoard log : {tb_error}")
                print(f"   ‚Üí L'entra√Ænement continuera mais peut √©chouer si l'ancien r√©pertoire n'existe plus")
            
            # Option : R√©initialiser le curriculum si demand√©
            if reset_curriculum:
                print("üîÑ R√©initialisation du curriculum √† z√©ro")
                if USE_VECENV and NUM_PARALLEL_ENVS > 1:
                    # VecEnv : reset tous les environnements
                    if USE_SUBPROC_VECENV:
                        # SubprocVecEnv : utiliser set_attr avec chemin d'attribut pour modifier les environnements dans les processus s√©par√©s
                        # Note: set_attr avec chemin d'attribut 'env.attribut' pour acc√©der √† AeroPatrolWrapper via Monitor
                        try:
                            # Essayer d'utiliser set_attr avec chemin d'attribut
                            env.set_attr('env.current_stage', CURRICULUM_START_STAGE, indices=None)
                            env.set_attr('env.episode_count', 0, indices=None)
                            env.set_attr('env.episode_count_per_stage', {0: 0, 1: 0, 2: 0}, indices=None)
                            env.set_attr('env.timesteps_count_per_stage', {0: 0, 1: 0, 2: 0}, indices=None)
                            env.set_attr('env.recent_episodes_success', [], indices=None)
                            env.set_attr('env.recent_episodes_detection', [], indices=None)
                            env.set_attr('env.recent_episodes_tracking', [], indices=None)
                            env.set_attr('env.recent_episodes_rewards', [], indices=None)
                        except (AttributeError, TypeError) as e:
                            # Si set_attr avec chemin ne fonctionne pas, utiliser une m√©thode via call_method si disponible
                            # Sinon, les environnements seront reset au prochain √©pisode
                            print(f"   ‚ö†Ô∏è  set_attr avec chemin d'attribut non support√©: {e}")
                            print("   ‚ö†Ô∏è  Les environnements seront reset au prochain √©pisode (reset automatique)")
                            # Note: Les environnements seront automatiquement reset au prochain reset() avec le stage correct
                    else:
                        # DummyVecEnv : acc√®s direct via env.envs
                        for i in range(NUM_PARALLEL_ENVS):
                            wrapped_env = env.envs[i]
                            if hasattr(wrapped_env, 'env'):
                                aero_env = wrapped_env.env
                                if hasattr(aero_env, 'current_stage'):
                                    aero_env.current_stage = CURRICULUM_START_STAGE
                                    aero_env.episode_count = 0
                                    aero_env.episode_count_per_stage = {0: 0, 1: 0, 2: 0}
                                    aero_env.timesteps_count_per_stage = {0: 0, 1: 0, 2: 0}
                                    aero_env.recent_episodes_success = []
                                    aero_env.recent_episodes_detection = []
                                    aero_env.recent_episodes_tracking = []
                                    aero_env.recent_episodes_rewards = []
                    print(f"   ‚Üí Stage r√©initialis√© √† {CURRICULUM_START_STAGE} pour tous les environnements")
                else:
                    # Environnement unique
                    env.env.current_stage = CURRICULUM_START_STAGE
                    env.env.episode_count = 0
                    env.env.episode_count_per_stage = {0: 0, 1: 0, 2: 0}
                    env.env.timesteps_count_per_stage = {0: 0, 1: 0, 2: 0}
                    env.env.recent_episodes_success = []
                    env.env.recent_episodes_detection = []
                    env.env.recent_episodes_tracking = []
                    env.env.recent_episodes_rewards = []
                    print(f"   ‚Üí Stage r√©initialis√© √† {CURRICULUM_START_STAGE}")
            else:
                # Tenter de restaurer l'√©tat du curriculum depuis un fichier JSON
                curriculum_state_path = load_model_path.replace('.zip', '_curriculum_state.json')
                if os.path.exists(curriculum_state_path):
                    try:
                        with open(curriculum_state_path, 'r') as f:
                            curriculum_state = json.load(f)
                        
                        if USE_VECENV and NUM_PARALLEL_ENVS > 1:
                            # VecEnv : restaurer pour tous les environnements
                            if USE_SUBPROC_VECENV:
                                # SubprocVecEnv : utiliser set_attr avec chemin d'attribut pour restaurer l'√©tat
                                # S'assurer que tous les dictionnaires ont toutes les cl√©s (0, 1, 2)
                                default_episode_count = {0: 0, 1: 0, 2: 0}
                                default_timesteps_count = {0: 0, 1: 0, 2: 0}
                                episode_count_loaded = curriculum_state.get('episode_count_per_stage', default_episode_count)
                                timesteps_count_loaded = curriculum_state.get('timesteps_count_per_stage', default_timesteps_count)
                                
                                # Fusionner avec les valeurs par d√©faut pour garantir toutes les cl√©s
                                episode_count_merged = {**default_episode_count, **episode_count_loaded}
                                timesteps_count_merged = {**default_timesteps_count, **timesteps_count_loaded}
                                
                                try:
                                    env.set_attr('env.current_stage', curriculum_state.get('current_stage', CURRICULUM_START_STAGE), indices=None)
                                    env.set_attr('env.episode_count', curriculum_state.get('episode_count', 0), indices=None)
                                    env.set_attr('env.episode_count_per_stage', episode_count_merged, indices=None)
                                    env.set_attr('env.timesteps_count_per_stage', timesteps_count_merged, indices=None)
                                    env.set_attr('env.recent_episodes_success', curriculum_state.get('recent_episodes_success', []), indices=None)
                                    env.set_attr('env.recent_episodes_detection', curriculum_state.get('recent_episodes_detection', []), indices=None)
                                    env.set_attr('env.recent_episodes_tracking', curriculum_state.get('recent_episodes_tracking', []), indices=None)
                                    env.set_attr('env.recent_episodes_rewards', curriculum_state.get('recent_episodes_rewards', []), indices=None)
                                except (AttributeError, TypeError) as e:
                                    print(f"   ‚ö†Ô∏è  set_attr avec chemin d'attribut non support√©: {e}")
                                    print("   ‚ö†Ô∏è  L'√©tat du curriculum ne peut pas √™tre restaur√© automatiquement avec SubprocVecEnv")
                                    print("   ‚ÑπÔ∏è  Les environnements utiliseront l'√©tat par d√©faut (Stage 0)")
                            else:
                                # DummyVecEnv : acc√®s direct via env.envs
                                for i in range(NUM_PARALLEL_ENVS):
                                    wrapped_env = env.envs[i]
                                    if hasattr(wrapped_env, 'env'):
                                        aero_env = wrapped_env.env
                                        if hasattr(aero_env, 'current_stage'):
                                            aero_env.current_stage = curriculum_state.get('current_stage', CURRICULUM_START_STAGE)
                                            aero_env.episode_count = curriculum_state.get('episode_count', 0)
                                            aero_env.episode_count_per_stage = curriculum_state.get('episode_count_per_stage', {0: 0, 1: 0, 2: 0})
                                            aero_env.timesteps_count_per_stage = curriculum_state.get('timesteps_count_per_stage', {0: 0, 1: 0, 2: 0})
                                            aero_env.recent_episodes_success = curriculum_state.get('recent_episodes_success', [])
                                            aero_env.recent_episodes_detection = curriculum_state.get('recent_episodes_detection', [])
                                            aero_env.recent_episodes_tracking = curriculum_state.get('recent_episodes_tracking', [])
                                            aero_env.recent_episodes_rewards = curriculum_state.get('recent_episodes_rewards', [])
                            print("‚úÖ √âtat du curriculum restaur√© depuis le fichier (tous les environnements)")
                            print(f"   ‚Üí Stage actuel : {curriculum_state.get('current_stage', CURRICULUM_START_STAGE)}")
                            print(f"   ‚Üí √âpisodes totaux : {curriculum_state.get('episode_count', 0)}")
                        else:
                            # Environnement unique
                            env.env.current_stage = curriculum_state.get('current_stage', CURRICULUM_START_STAGE)
                            env.env.episode_count = curriculum_state.get('episode_count', 0)
                            
                            # S'assurer que tous les dictionnaires ont toutes les cl√©s (0, 1, 2)
                            default_episode_count = {0: 0, 1: 0, 2: 0}
                            default_timesteps_count = {0: 0, 1: 0, 2: 0}
                            episode_count_loaded = curriculum_state.get('episode_count_per_stage', default_episode_count)
                            timesteps_count_loaded = curriculum_state.get('timesteps_count_per_stage', default_timesteps_count)
                            
                            # Fusionner avec les valeurs par d√©faut pour garantir toutes les cl√©s
                            env.env.episode_count_per_stage = {**default_episode_count, **episode_count_loaded}
                            env.env.timesteps_count_per_stage = {**default_timesteps_count, **timesteps_count_loaded}
                            
                            env.env.recent_episodes_success = curriculum_state.get('recent_episodes_success', [])
                            env.env.recent_episodes_detection = curriculum_state.get('recent_episodes_detection', [])
                            env.env.recent_episodes_tracking = curriculum_state.get('recent_episodes_tracking', [])
                            env.env.recent_episodes_rewards = curriculum_state.get('recent_episodes_rewards', [])
                            print("‚úÖ √âtat du curriculum restaur√© depuis le fichier")
                            print(f"   ‚Üí Stage actuel : {env.env.current_stage}")
                            print(f"   ‚Üí √âpisodes totaux : {env.env.episode_count}")
                            print(f"   ‚Üí √âpisodes par stage : {env.env.episode_count_per_stage}")
                            print(f"   ‚Üí Timesteps par stage : {env.env.timesteps_count_per_stage}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  Impossible de restaurer l'√©tat du curriculum : {e}")
                        print("   ‚Üí Utilisation de l'√©tat par d√©faut")
                else:
                    print("‚ÑπÔ∏è  Aucun fichier d'√©tat du curriculum trouv√©, utilisation de l'√©tat par d√©faut")
                    if USE_VECENV and NUM_PARALLEL_ENVS > 1:
                        # Pour VecEnv, on ne peut pas acc√©der facilement au stage, donc on affiche juste un message
                        print(f"   ‚Üí Stage par d√©faut : {CURRICULUM_START_STAGE} (pour tous les environnements)")
                    else:
                        print(f"   ‚Üí Stage actuel : {env.env.current_stage}")
                        print(f"   ‚Üí √âpisodes totaux : {env.env.episode_count}")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
            return
    else:
        # Cr√©er un nouveau mod√®le
        print("üÜï Cr√©ation d'un nouveau mod√®le PPO")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=LEARNING_RATE,
            gamma=GAMMA,
            verbose=1,
            tensorboard_log=session_log_dir,  # ‚úÖ Activation de TensorBoard
            device=device,  # ‚úÖ Utilisation explicite du GPU si disponible
            max_grad_norm=0.5,  # ‚úÖ Gradient clipping pour stabiliser (Run 7: loss tr√®s √©lev√©e)
            seed=SEED,  # ‚úÖ Graine pour reproductibilit√©
            ent_coef=0.01,  # ‚úÖ Bonus d'entropie pour encourager l'exploration (√©vite convergence pr√©matur√©e)
            # Hyperparam√®tres ajust√©s pour am√©liorer la stabilit√© (CV) et ep_rew_mean (2025-12-07)
            n_steps=3072,  # Augment√© de 2560 √† 3072 (+20%) pour am√©liorer la stabilit√© et ep_rew_mean
            batch_size=96,  # Augment√© de 80 √† 96 (+20%) pour gradients plus stables et meilleure convergence
            n_epochs=10,  # Augment√© de 8 √† 10 (+25%) pour meilleure optimisation (√©tait 12, r√©duit pour mises √† jour plus fr√©quentes)
            gae_lambda=0.98,  # Conserv√© √† 0.98 (meilleure estimation de la valeur)
            vf_coef=0.5,  # Augment√© de 0.5 (d√©faut) pour am√©liorer l'apprentissage de la value function (r√©duire value_loss)
        )
    
    # ======================================================================
    # üíæ 5. CONFIGURATION DES CALLBACKS (TensorBoard + Checkpoint)
    # ======================================================================
    # Cr√©e le callback TensorBoard pour m√©triques personnalis√©es
    tensorboard_callback = TensorBoardCallback()
    
    # Cr√©e le callback de checkpoint pour sauvegarder p√©riodiquement
    checkpoint_dir = os.path.join(current_dir, SAVE_DIR, CHECKPOINT_DIR)
    ensure_dir(checkpoint_dir)
    checkpoint_callback = CheckpointCallback(
        save_freq=CHECKPOINT_SAVE_FREQ,
        save_path=checkpoint_dir,
        name_prefix="ppo_marl_checkpoint",
        save_replay_buffer=False,  # Ne pas sauvegarder le replay buffer (√©conomise de l'espace)
        save_vecnormalize=False,  # Pas de VecNormalize utilis√©
    )
    
    # üéì Callback personnalis√© pour sauvegarder l'√©tat du curriculum √† chaque checkpoint
    class CurriculumStateCallback(BaseCallback):
        """Sauvegarde l'√©tat du curriculum √† chaque checkpoint."""
        def __init__(self, env, checkpoint_dir, verbose=0):
            super().__init__(verbose)
            self.env = env
            self.checkpoint_dir = checkpoint_dir
        
        def _get_curriculum_state(self):
            """R√©cup√®re l'√©tat du curriculum depuis l'environnement (compatible VecEnv et environnement unique)."""
            # V√©rifier si c'est un VecEnv (DummyVecEnv ou SubprocVecEnv)
            if hasattr(self.env, 'envs'):  # DummyVecEnv
                # Acc√©der au premier environnement (tous les environnements partagent le m√™me √©tat de curriculum)
                wrapped_env = self.env.envs[0]
                if hasattr(wrapped_env, 'env'):
                    aero_env = wrapped_env.env
                    if hasattr(aero_env, 'current_stage'):
                        return {
                            'current_stage': aero_env.current_stage,
                            'episode_count': aero_env.episode_count,
                            'episode_count_per_stage': aero_env.episode_count_per_stage.copy() if hasattr(aero_env, 'episode_count_per_stage') else {},
                            'timesteps_count_per_stage': aero_env.timesteps_count_per_stage.copy() if hasattr(aero_env, 'timesteps_count_per_stage') else {},
                            'recent_episodes_success': aero_env.recent_episodes_success.copy() if hasattr(aero_env, 'recent_episodes_success') else [],
                            'recent_episodes_detection': aero_env.recent_episodes_detection.copy() if hasattr(aero_env, 'recent_episodes_detection') else [],
                            'recent_episodes_tracking': aero_env.recent_episodes_tracking.copy() if hasattr(aero_env, 'recent_episodes_tracking') else [],
                            'recent_episodes_rewards': aero_env.recent_episodes_rewards.copy() if hasattr(aero_env, 'recent_episodes_rewards') else []
                        }
            elif hasattr(self.env, 'get_attr'):  # SubprocVecEnv
                # Utiliser get_attr pour acc√©der aux attributs dans les processus s√©par√©s
                try:
                    current_stage = self.env.get_attr('env.current_stage', indices=[0])[0]
                    episode_count = self.env.get_attr('env.episode_count', indices=[0])[0]
                    episode_count_per_stage = self.env.get_attr('env.episode_count_per_stage', indices=[0])[0]
                    timesteps_count_per_stage = self.env.get_attr('env.timesteps_count_per_stage', indices=[0])[0]
                    recent_episodes_success = self.env.get_attr('env.recent_episodes_success', indices=[0])[0]
                    recent_episodes_detection = self.env.get_attr('env.recent_episodes_detection', indices=[0])[0]
                    recent_episodes_tracking = self.env.get_attr('env.recent_episodes_tracking', indices=[0])[0]
                    recent_episodes_rewards = self.env.get_attr('env.recent_episodes_rewards', indices=[0])[0]
                    return {
                        'current_stage': current_stage,
                        'episode_count': episode_count,
                        'episode_count_per_stage': episode_count_per_stage.copy() if isinstance(episode_count_per_stage, dict) else {},
                        'timesteps_count_per_stage': timesteps_count_per_stage.copy() if isinstance(timesteps_count_per_stage, dict) else {},
                        'recent_episodes_success': recent_episodes_success.copy() if isinstance(recent_episodes_success, list) else [],
                        'recent_episodes_detection': recent_episodes_detection.copy() if isinstance(recent_episodes_detection, list) else [],
                        'recent_episodes_tracking': recent_episodes_tracking.copy() if isinstance(recent_episodes_tracking, list) else [],
                        'recent_episodes_rewards': recent_episodes_rewards.copy() if isinstance(recent_episodes_rewards, list) else []
                    }
                except Exception as e:
                    if self.verbose > 0:
                        print(f"‚ö†Ô∏è  Impossible de r√©cup√©rer l'√©tat du curriculum depuis SubprocVecEnv : {e}")
                    return None
            else:  # Environnement unique
                if hasattr(self.env, 'env') and hasattr(self.env.env, 'current_stage'):
                    return {
                        'current_stage': self.env.env.current_stage,
                        'episode_count': self.env.env.episode_count,
                        'episode_count_per_stage': self.env.env.episode_count_per_stage.copy() if hasattr(self.env.env, 'episode_count_per_stage') else {},
                        'timesteps_count_per_stage': self.env.env.timesteps_count_per_stage.copy() if hasattr(self.env.env, 'timesteps_count_per_stage') else {},
                        'recent_episodes_success': self.env.env.recent_episodes_success.copy() if hasattr(self.env.env, 'recent_episodes_success') else [],
                        'recent_episodes_detection': self.env.env.recent_episodes_detection.copy() if hasattr(self.env.env, 'recent_episodes_detection') else [],
                        'recent_episodes_tracking': self.env.env.recent_episodes_tracking.copy() if hasattr(self.env.env, 'recent_episodes_tracking') else [],
                        'recent_episodes_rewards': self.env.env.recent_episodes_rewards.copy() if hasattr(self.env.env, 'recent_episodes_rewards') else []
                    }
            return None
        
        def _on_step(self) -> bool:
            # Sauvegarder l'√©tat du curriculum √† chaque checkpoint (m√™me fr√©quence que CheckpointCallback)
            if self.n_calls % CHECKPOINT_SAVE_FREQ == 0:
                curriculum_state = self._get_curriculum_state()
                if curriculum_state is not None:
                    # Sauvegarder avec le m√™me nom que le checkpoint (sans extension .zip)
                    checkpoint_name = f"ppo_marl_checkpoint_{self.num_timesteps}_steps"
                    curriculum_state_path = os.path.join(self.checkpoint_dir, f"{checkpoint_name}_curriculum_state.json")
                    with open(curriculum_state_path, 'w') as f:
                        json.dump(curriculum_state, f, indent=2)
                    if self.verbose > 0:
                        print(f"üíæ √âtat du curriculum sauvegard√© : {curriculum_state_path}")
            return True
    
    curriculum_callback = CurriculumStateCallback(env, checkpoint_dir, verbose=1)
    
    # Combiner les callbacks
    callback = CallbackList([tensorboard_callback, checkpoint_callback, curriculum_callback])
    
    print(f"üíæ Checkpoints sauvegard√©s toutes les {CHECKPOINT_SAVE_FREQ:,} steps dans : {checkpoint_dir}")
    
    # Apprentissage
    total_timesteps = EPISODES * MAX_STEPS_PER_EPISODE
    print(f"üöÄ Training PPO for {total_timesteps:,} timesteps...")
    print(f"üìä TensorBoard disponible dans : {session_log_dir}")
    print(f"üí° Pour visualiser : tensorboard --logdir {session_log_dir}")
    
    # D√©marrer TensorBoard automatiquement (optionnel)
    # Note : Si le port est d√©j√† utilis√©, TensorBoard ne d√©marrera pas mais l'entra√Ænement continuera
    try:
        start_tensorboard(TENSORBOARD_LOG_DIR, port=6006)
        print(f"üí° TensorBoard est d√©marr√© automatiquement. Vous pouvez le fermer, les m√©triques seront toujours sauvegard√©es.")
        print(f"üí° Pour relancer TensorBoard plus tard : tensorboard --logdir {session_log_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Impossible de d√©marrer TensorBoard automatiquement : {e}")
        print(f"   ‚Üí L'entra√Ænement continuera normalement, les m√©triques seront sauvegard√©es dans les fichiers CSV")
        print(f"   ‚Üí Vous pouvez d√©marrer TensorBoard manuellement avec : tensorboard --logdir {session_log_dir}")
    
    # D√©sactiver la barre de progression si tqdm/rich ne sont pas install√©s
    try:
        import tqdm
        import rich
        use_progress_bar = True
    except ImportError:
        print("‚ö†Ô∏è  tqdm/rich non install√©s. Barre de progression d√©sactiv√©e.")
        print("   Pour l'activer : pip install tqdm rich")
        use_progress_bar = False
    
    # Sauvegarde d'urgence en cas d'interruption
    import signal
    import atexit
    
    def save_emergency_model():
        """Sauvegarde d'urgence du mod√®le en cas de crash ou interruption."""
        try:
            emergency_path = os.path.join(SAVE_DIR, f"{MODEL_NAME}_emergency_{timestamp}")
            model.save(emergency_path)
            print(f"\nüíæ Mod√®le d'urgence sauvegard√© : {emergency_path}.zip")
        except Exception as e:
            print(f"‚ö†Ô∏è  Impossible de sauvegarder le mod√®le d'urgence : {e}")
    
    # Enregistrer la fonction de sauvegarde d'urgence
    atexit.register(save_emergency_model)
    
    def signal_handler(signum, frame):
        """G√®re les signaux d'interruption (Ctrl+C, etc.)."""
        print(f"\n‚ö†Ô∏è  Interruption d√©tect√©e (signal {signum})")
        save_emergency_model()
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    print("üîç [DIAGNOSTIC] D√©but de model.learn()...")
    sys.stdout.flush()
    
    try:
        print("üîç [DIAGNOSTIC] Appel de model.learn() avec les param√®tres suivants :")
        print(f"   ‚Üí total_timesteps: {total_timesteps:,}")
        print(f"   ‚Üí callback: {type(callback).__name__}")
        print(f"   ‚Üí progress_bar: {use_progress_bar}")
        sys.stdout.flush()
        
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,  # ‚úÖ Callback pour m√©triques personnalis√©es + checkpoints
            progress_bar=use_progress_bar
        )
        
        print("üîç [DIAGNOSTIC] model.learn() termin√© avec succ√®s")
        sys.stdout.flush()
    except KeyboardInterrupt:
        print(f"\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur")
        save_emergency_model()
        raise
    except (ConnectionError, TimeoutError, OSError) as e:
        print(f"\n‚ùå Erreur de connexion Unity : {e}")
        print(f"   ‚Üí V√©rifiez que les builds Unity sont toujours actifs")
        print(f"   ‚Üí Utilisez : .\\check_unity_builds.ps1")
        save_emergency_model()
        raise
    except FileNotFoundError as e:
        print(f"\n‚ùå Erreur de fichier (TensorBoard) : {e}")
        print(f"   ‚Üí Tentative de correction...")
        try:
            # R√©initialiser le logger TensorBoard
            if hasattr(model, 'logger'):
                if hasattr(model.logger, 'close'):
                    model.logger.close()
                from stable_baselines3.common.logger import configure
                model.logger = configure(session_log_dir, ["stdout", "csv", "tensorboard"])
            print(f"   ‚Üí Logger TensorBoard r√©initialis√©")
            print(f"   ‚Üí Relancez l'entra√Ænement")
        except Exception as fix_error:
            print(f"   ‚Üí Impossible de corriger : {fix_error}")
        save_emergency_model()
        raise
    except Exception as e:
        print(f"\n‚ùå Erreur pendant l'entra√Ænement : {e}")
        import traceback
        print(f"\nüìã Traceback complet :")
        traceback.print_exc()
        save_emergency_model()
        raise

    # ======================================================================
    # üíæ 4. SAUVEGARDE DU MOD√àLE
    # ======================================================================
    # Sauvegarde du mod√®le avec timestamp pour √©viter d'√©craser les pr√©c√©dents
    model_path_timestamped = os.path.join(SAVE_DIR, f"{MODEL_NAME}_{timestamp}")
    model.save(model_path_timestamped)
    print(f"‚úÖ Model saved at {model_path_timestamped}.zip")
    
    # Sauvegarder l'√©tat du curriculum avec le mod√®le
    try:
        if hasattr(env, 'env') and hasattr(env.env, 'get_curriculum_state'):
            curriculum_state = env.env.get_curriculum_state()
            if curriculum_state:
                curriculum_state_path = f"{model_path_timestamped}_curriculum_state.json"
                with open(curriculum_state_path, 'w', encoding='utf-8') as f:
                    json.dump(curriculum_state, f, indent=2, ensure_ascii=False)
                print(f"‚úÖ √âtat du curriculum sauvegard√© : {curriculum_state_path}")
        elif hasattr(env, 'get_attr'):  # VecEnv
            try:
                curriculum_state = curriculum_callback._get_curriculum_state()
                if curriculum_state:
                    curriculum_state_path = f"{model_path_timestamped}_curriculum_state.json"
                    with open(curriculum_state_path, 'w', encoding='utf-8') as f:
                        json.dump(curriculum_state, f, indent=2, ensure_ascii=False)
                    print(f"‚úÖ √âtat du curriculum sauvegard√© : {curriculum_state_path}")
            except:
                pass
    except Exception as e:
        print(f"‚ö†Ô∏è  Impossible de sauvegarder l'√©tat du curriculum : {e}")
    
    # Sauvegarde aussi sous le nom standard (dernier mod√®le)
    model_path_standard = os.path.join(SAVE_DIR, MODEL_NAME)
    model.save(model_path_standard)
    print(f"‚úÖ Model also saved as {model_path_standard}.zip (latest model)")
    
    # Sauvegarde du mod√®le dans le dossier d'exp√©rimentation
    experiment_model_path = os.path.join(experiment_dir, f"{MODEL_NAME}_{timestamp}.zip")
    if os.path.exists(f"{model_path_timestamped}.zip"):
        shutil.copy2(f"{model_path_timestamped}.zip", experiment_model_path)
        print(f"‚úÖ Model also saved in experiment dir : {experiment_model_path}")
    else:
        print(f"‚ö†Ô∏è  Mod√®le non trouv√© pour copie dans experiment dir : {model_path_timestamped}.zip")
    
    # ======================================================================
    # üìä 5. SAUVEGARDE DES INFORMATIONS D'EXP√âRIMENTATION
    # ======================================================================
    model_info = {
        "model_name": MODEL_NAME,
        "model_path_timestamped": f"{model_path_timestamped}.zip",
        "model_path_standard": f"{model_path_standard}.zip",
        "model_path_experiment": experiment_model_path,
        "total_timesteps": EPISODES * MAX_STEPS_PER_EPISODE,
        "episodes": EPISODES,
        "max_steps_per_episode": MAX_STEPS_PER_EPISODE,
        "learning_rate": LEARNING_RATE,
        "gamma": GAMMA,
        "device": device,
        "num_drones": NUM_DRONES,
    }
    
    save_experiment_info(experiment_dir, config_dict, model_info, timestamp)
    
    # ======================================================================
    # üìä 6. G√âN√âRATION DU RAPPORT D'ANALYSE
    # ======================================================================
    generate_analysis_report(experiment_dir, timestamp, env)
    
    # ======================================================================
    # üéØ 7. R√âSUM√â DE L'EXP√âRIMENTATION
    # ======================================================================
    print("\n" + "=" * 70)
    print("‚úÖ EXP√âRIMENTATION TERMIN√âE AVEC SUCC√àS")
    print("=" * 70)
    print(f"üìÅ Dossier d'exp√©rimentation : {experiment_dir}")
    print(f"ü§ñ Mod√®le sauvegard√© : {model_path_timestamped}.zip")
    print(f"üìä Logs TensorBoard : {session_log_dir}")
    print(f"üìù Configuration : {config_path}")
    print(f"üì¶ Fichiers archiv√©s : {len(archived_files)} fichiers")
    print("=" * 70)
    print(f"\nüí° Pour visualiser TensorBoard : tensorboard --logdir {session_log_dir}")
    print(f"üí° Pour √©valuer le mod√®le : python evaluate_marl.py --model {MODEL_NAME}_{timestamp}.zip")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Entra√Ænement PPO pour patrouille multi-drone",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:
  # Nouvel entra√Ænement
  python train_marl.py
  
  # Continuer un entra√Ænement existant (recommand√©)
  python train_marl.py --continue_training models/ppo_marl_20251204_115027.zip
  
  # OU utiliser --load_model (m√™me effet que --continue_training)
  python train_marl.py --load_model models/ppo_marl_20251204_115027.zip
  
  # Repartir de z√©ro avec un mod√®le existant (curriculum r√©initialis√©)
  python train_marl.py --load_model models/ppo_marl_20251204_115027.zip --reset_curriculum
        """
    )
    parser.add_argument(
        '--load_model',
        type=str,
        default=None,
        help='Chemin vers un mod√®le √† charger (ex: models/ppo_marl_20251204_115027.zip). Restaure automatiquement l\'√©tat du curriculum si disponible.'
    )
    parser.add_argument(
        '--continue_training',
        type=str,
        default=None,
        metavar='MODEL_PATH',
        help='Continuer un entra√Ænement existant (alias de --load_model). Restaure automatiquement l\'√©tat du curriculum.'
    )
    parser.add_argument(
        '--reset_curriculum',
        action='store_true',
        help='R√©initialiser le curriculum √† z√©ro (utilis√© avec --load_model ou --continue_training)'
    )
    
    args = parser.parse_args()
    
    # --continue_training est un alias de --load_model
    load_model_path = args.continue_training if args.continue_training else args.load_model
    
    train(load_model_path=load_model_path, reset_curriculum=args.reset_curriculum)
